Write a python program to demonstrate bitwise operation

def bitwise_operation(a, b):
    # Bitwise AND
    bitwise_and_result = a & b
    print(f"Bitwise AND: {bitwise_and_result}")
    
    # Bitwise OR
    bitwise_or_result = a | b
    print(f"Bitwise OR: {bitwise_or_result}")
    
    # Bitwise XOR
    bitwise_xor_result = a ^ b
    print(f"Bitwise XOR: {bitwise_xor_result}")
    
    # Bitwise NOT (for a and b)
    bitwise_not_result_a = ~a
    bitwise_not_result_b = ~b
    print(f"Bitwise NOT of a: {bitwise_not_result_a}")
    print(f"Bitwise NOT of b: {bitwise_not_result_b}")
    
    # Left shift (a << 1)
    left_shift = a << 1
    print(f"Left Shift of a: {left_shift}")
    
    # Right shift (a >> 1)
    right_shift = a >> 1
    print(f"Right Shift of a: {right_shift}")

print("Bitwise Operation:")
bitwise_operation(0, 1)
---------------------------------------------------------------------------------------------------------------

AIM: Write a python code for implementation PageRank using NetworkX.

1. Without Weighted Edges:

import networkx as nx
import pylab as plt

# Create a directed graph
G = nx.DiGraph()
G.add_nodes_from(["A", "B", "C", "E", "F", "G"])
G.add_edges_from([("A", "G"), ("G", "A"), ("F", "A"), ("E", "A"), ("A", "D"), ("A", "C"), ("D", "B"),
                  ("B", "A"), ("A", "C"), ("D", "F")])

# Calculate PageRank
ppr1 = nx.pagerank(G)
print("PageRank values:", ppr1)

# Draw the graph
pos = nx.spring_layout(G)  # Choose layout
nx.draw_networkx(G, pos, with_labels=True, node_color="#f86e00")
plt.show()

2. With Weighted Edges:

import networkx as nx
import pylab as plt

# Create a directed graph with weighted edges
D = nx.DiGraph()
D.add_weighted_edges_from([('A', 'B', 1), ('A', 'C', 1), ('B', 'A', 1), ('C', 'A', 1)])

# Calculate PageRank
ppr2 = nx.pagerank(D)
print("PageRank values:", ppr2)

# Draw the graph
pos = nx.spring_layout(D)  # Choose layout
nx.draw_networkx(D, pos, with_labels=True, node_color="#f86e00")
plt.show()


3. Custom PageRank Implementation:

def page_rank(graph, damping_factor=0.85, max_iterations=100, tolerance=1e-6):
    num_pages = len(graph)
    initial_page_rank = 1 / num_pages
    page_ranks = {page: initial_page_rank for page in graph}

    for _ in range(max_iterations):
        new_page_ranks = {}

        for page in graph:
            new_rank = (1 - damping_factor) / num_pages

            for link in graph:
                if page in graph[link]:
                    new_rank += damping_factor * (page_ranks[link] / len(graph[link]))

            new_page_ranks[page] = new_rank

        convergence = all(abs(new_page_ranks[page] - page_ranks[page]) < tolerance for page in graph)
        page_ranks = new_page_ranks

        if convergence:
            break

    return page_ranks

if __name__ == "__main__":
    example_graph = {
        'A': ['B', 'C'],
        'B': ['A'],
        'C': ['A', 'B'],
        'D': ['B']
    }

    result = page_rank(example_graph)
    for page, rank in sorted(result.items(), key=lambda x: x[1], reverse=True):
        print(f"Page: {page} - PageRank: {rank:.4f}")

-------------------------------------------------------------------------------------------------------------------------

AIM: Write a program to implement Levenshtein Distance.

def leven(x, y):
    n = len(x)
    m = len(y)
    
    # Initialize the 2D array A with dimensions (n+1) x (m+1)
    A = [[i + j for j in range(m + 1)] for i in range(n + 1)]
    
    # Fill in the dynamic programming table
    for i in range(n):
        for j in range(m):
            cost = 0 if x[i] == y[j] else 1
            A[i + 1][j + 1] = min(A[i][j + 1] + 1,      # Deletion
                                  A[i + 1][j] + 1,      # Insertion
                                  A[i][j] + cost)       # Substitution
    
    # The value at A[n][m] represents the Levenshtein distance
    return A[n][m]

# Test cases
print(leven("brap", "rap"))    # Output: 2
print(leven("trial", "try"))   # Output: 2
print(leven("horse", "force")) # Output: 1
print(leven("rose", "erode"))  # Output: 3

--------------------------------------------------------------------------------------------------------------------------

AIM: Write a program to Compute Similarity between two text documents.

Method 1: spaCy Similarity

import spacy
from spacy.cli.download import download
download(model="en_core_web_sm")

nlp = spacy.load('en_core_web_sm')

doc1 = nlp(u'Hello hi there!')
doc2 = nlp(u'Hello hi there!')
doc3 = nlp(u'Hey whatsup?')

similarity_doc1_doc2 = doc1.similarity(doc2)
similarity_doc1_doc3 = doc1.similarity(doc3)

print(f"Similarity between doc1 and doc2: {similarity_doc1_doc2}")
print(f"Similarity between doc1 and doc3: {similarity_doc1_doc3}")


Method 2: Jaccard Similarity

def jaccard_similarity(doc1, doc2):
    words_doc1 = set(doc1.lower().split())
    words_doc2 = set(doc2.lower().split())
    
    intersection = words_doc1.intersection(words_doc2)
    union = words_doc1.union(words_doc2)
    
    return float(len(intersection)) / len(union)

doc1 = "data is the new oil of the digital economy"
doc2 = "data is the new oil"

jaccard_similarity_score = jaccard_similarity(doc1, doc2)
print(f"Jaccard Similarity between doc1 and doc2: {jaccard_similarity_score}")

Method 3: Cosine Similarity with TF-IDF Vectorization

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

doc_1 = "Data is the new oil of the digital economy"
doc_2 = "Data is a new oil"

data = [doc_1, doc_2]

# Initialize the TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Transform the data into TF-IDF vectors
tfidf_matrix = tfidf_vectorizer.fit_transform(data)

# Get feature names (tokens)
tokens = tfidf_vectorizer.get_feature_names_out()

# Compute cosine similarity matrix
cosine_similarity_matrix = cosine_similarity(tfidf_matrix)

# Print cosine similarity matrix and corresponding document names
print(f"Cosine Similarity between doc_1 and doc_2: {cosine_similarity_matrix[0][1]}")
print("Document Names: ['doc_1', 'doc_2']")

------------------------------------------------------------------------------------------------------

AIM: Write a python program for map reducer.

from functools import reduce
from collections import defaultdict

def mapper(data):
    char_count = defaultdict(int)
    for char in data:
        if char.isalpha():
            char_count[char.lower()] += 1
    return char_count.items()

def reducer(counts1, counts2):
    merged_counts = defaultdict(int)
    for char, count in counts1:
        merged_counts[char] += count
    for char, count in counts2:
        merged_counts[char] += count
    return merged_counts.items()

if __name__ == "__main__":
    dataset = "Hello,World! This is a MapReduce example"
    
    # Split the dataset into chunks (assuming a distributed environment)
    chunks = [chunk for chunk in dataset.split()]
    
    # Map step
    mapped_results = map(mapper, chunks)
    
    # Reduce step
    final_counts = reduce(reducer, mapped_results)
    
    # Print the result
    for char, count in final_counts:
        print(f"Character: {char}, Count: {count}")

-----------------------------------------------------------------------------------------------------------

AIM: Write a python program for HITS Algorithm

# HITS Algorithm
import networkx as nx

# Step 2: Create a graph and add edges
G = nx.DiGraph()
G.add_edges_from([(1, 2), (1, 3), (2, 4), (3, 4), (4, 5)])

# Step 3: Calculate the HITS scores
authority_scores, hub_scores = nx.hits(G)

# Step 4: Print the scores
print("Authority Scores:", authority_scores)
print("Hub Scores:", hub_scores)

-----------------------------------------------------------------------------------------------------------

AIM: Write a python program for Stop Words.

# NLTK Stopwords
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# Method 7A: Get the stopwords
stopwords_english = set(stopwords.words('english'))
print(stopwords_english)

# Method 7B: Filter stopwords from a sentence
example_sent = "This is sample Sentence, showing off the stop words filtration."
word_tokens = word_tokenize(example_sent)

filtered_sentence = [w for w in word_tokens if not w in stopwords_english]
print("Word Tokens:", word_tokens)
print("Filtered Sentence:", filtered_sentence)

# Alternative way to filter stopwords
filtered_sentence = []
for w in word_tokens:
    if w not in stopwords_english:
        filtered_sentence.append(w)
print("Filtered Sentence:", filtered_sentence)

---------------------------------------------------------------------------------------------------------

AIM: Write a python program for Twitter Scraping using Nitter

!pip install ntscraper

import pandas as pd
from ntscraper import Nitter

scraper = Nitter()
tweets = scraper.get_tweets('narendramodi', mode='user', number=5)

final_tweets = []
for tweet in tweets['tweets']:
    data = [tweet['link'], tweet['text'], tweet['date'], tweet['stats']['likes'], tweet['stats']['comments']]
    final_tweets.append(data)

print(final_tweets)

data = pd.DataFrame(final_tweets, columns=['link', 'text', 'date', 'Number of likes', 'Number of comments'])
print(data)

--------------------------------------------------------------------------------------------------------------------

AIM:
Write a python program to implement simple web crawling.

import requests
from parsel import Selector
import time

start = time.time()
response = requests.get('http://recurship.com/')
selector = Selector(response.text)

href_links = selector.xpath('//a/@href').getall()
image_links = selector.xpath('//img/@src').getall()

print("*********************HREF_LINKS**********************")
print(href_links)
print("*********************/HREF_LINKS**********************")

print("*********************IMAGE_LINKS**********************")
print(image_links)
print("*********************/IMAGE_LINKS**********************")

------------------------------------------------------------------------------------------------------------

AIM:
to demonstrate the parsing of an XML document representing a web graph, generating a directed
graph (DiGraph) using the NetworkX library, and computing the topic-specific PageRank for specific
nodes within the graph.

import xml.etree.ElementTree as ET
import networkx as nx

def parse_xml(xml_text):
    root = ET.fromstring(xml_text)
    return root

def generate_web_graph(xml_root):
    G = nx.DiGraph()
    for page in xml_root.findall(".//page"):
        page_id = page.find("id").text
        G.add_node(page_id)
        links = page.findall(".//link")
        for link in links:
            target_page_id = link.text
            G.add_edge(page_id, target_page_id)
    return G

def compute_topic_specific_pagerank(graph, topic_nodes, alpha=0.85, max_iter=100, tol=1e-6):
    personalization = {node: 1.0 if node in topic_nodes else 0.0 for node in graph.nodes}
    return nx.pagerank(graph, alpha=alpha, personalization=personalization,
                       max_iter=max_iter, tol=tol)

if __name__ == "__main__":
    # Example XML text representing web pages and links
    example_xml = '''
    <webgraph>
    <page>
    <id>1</id>
    <link>2</link>
    <link>2</link>
    </page>
    <page>
    <id>2</id>
    <link>1</link>
    <link>3</link>
    </page>
    <page>
    <id>3</id>
    <link>1</link>
    <link>2</link>
    </page>
    </webgraph>
    '''
    # Parse XML
    xml_root = parse_xml(example_xml)
    # Generate web graph
    web_graph = generate_web_graph(xml_root)
    # Compute topic-specific PageRank for nodes 1 and 2
    topic_specific_pagerank = compute_topic_specific_pagerank(web_graph, topic_nodes=["1", "2"])
    # Print the result
    print("Topic-Specific PageRank:")
    for node, score in sorted(topic_specific_pagerank.items(), key=lambda x: x[1], reverse=True):
        print(f"Node {node} - PageRank: {score:.4f}")

-----------------------------------------------------------------------------------------------------------

A) AIM:
Write a python program to retrieve xml text using xml library.

import xml.etree.ElementTree as ET

xml_data = '''<root>
<person>
<Name>Mr.Rajesh Yadav</Name>
<name>John</name>
<age>30</age>
<city>New York</city>
</person>
<person>
<name>Alice</name>
<age>25</age>
<city>London</city>
</person>
</root>'''

tree = ET.fromstring(xml_data)

for person in tree.findall('person'):
    name = person.find('name').text
    age = person.find('age').text
    city = person.find('city').text
    print(f"Name: {name}, Age: {age}, City: {city}")

----------------------------------------------------------------------------------------------------------

AIM:
Write a python program to retrieve xml text using lxml library.

import xml.etree.ElementTree as ET

xml_data = '''<root>
<person>
<name>John</name>
Name of Instructor: Mr.Rajesh Yadav
<age>30</age>
<city>New York</city>
</person>
<person>
<name>Alice</name>
<age>25</age>
<city>London</city>
</person>
</root>'''

tree = ET.fromstring(xml_data)

for person in tree.findall('person'):
    name = person.find('name').text
    age = person.find('age').text
    city = person.find('city').text
    print(f"Name: {name}, Age: {age}, City: {city}")

---------------------------------------------------------------------------------------------------------
